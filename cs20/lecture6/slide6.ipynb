{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow中的卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在TensorFlow中去做卷积，我们有很多内建的层可以使用。你可以输入2维数据做1维卷积，输入3维数据做2维卷积，输入4维数据做3维卷积，最常用的是2维卷积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 函数模型\n",
    "tf.nn.conv2d(\n",
    "    input,\n",
    "    filter,\n",
    "    strides,\n",
    "    padding,\n",
    "    use_cudnn_on_gpu=True,\n",
    "    data_format='NHWC',\n",
    "    dilations=[1, 1, 1, 1],\n",
    "    name=None)\n",
    "\n",
    "Input: Batch size (N) x Height (H) x Width (W) x Channels (C)\n",
    "Filter: Height x Width x Input Channels x Output Channels\n",
    "(e.g. [5, 5, 3, 64])\n",
    "Strides: 4 element 1-D tensor, strides in each direction\n",
    "(often [1, 1, 1, 1] or [1, 2, 2, 1])\n",
    "Padding: 'SAME' or 'VALID'\n",
    "Dilations: The dilation factor. If set to k > 1, there will be k-1 skipped cells between each filter element on that dimension.\n",
    "Data_format: default to NHWC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作一个有趣的练习：GitHub中的kernes.py文件中看到一些著名的核的值，在07_run_kernels.py中看到它们的用法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用CNN处理MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第三课中学习了逻辑回归处理MNIST，现在我们使用CNN来处理，看看结果如何！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将采用如下架构：两个步长为1的卷积层，每个卷积层后跟一个relu激活层与最大池化层Maxpool，最后跟两个全连接层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.卷积层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 输入尺寸(W)\n",
    "- 过滤器尺寸(F)\n",
    "- 步长(S)\n",
    "- 零填充(P)\n",
    "\n",
    "在定义函数之前，让我们看一下获取输出大小的公式。当您具有上述输入值时，输出的大小如下所示：\n",
    "\n",
    "$$ \\frac{W-F+2P}{S}+1 $$\n",
    "\n",
    "\n",
    "在我们的MNIST模型中，输入为28x28，滤波器为5x5。并且步幅使用1和填充使用2。因此，输出的大小如下:\n",
    "\n",
    "$$ \\frac{28-5+2\\times2}{1}+1 = 28 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu(inputs, filters, k_size, stride, padding, scope_name):\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "        # rgb通道\n",
    "        in_channels = inputs.shape[-1]\n",
    "        # 卷积核\n",
    "        kernel = tf.get_variable('kernel', [k_size, k_size, in_channels, filters],\n",
    "                                initializer=tf.truncated_normal_initializer())\n",
    "        biases = tf.get_variable('biases', [filters],\n",
    "                            initializer=tf.random_normal_initializer())\n",
    "        # 卷积结果\n",
    "        conv = tf.nn.conv2d(inputs, kernel, strides=[1, stride, stride, 1], padding=padding)\n",
    "    # relu层对卷积结果处理\n",
    "    return tf.nn.relu(conv + biases, name=scope.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.池化层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "池化可减少要素图的维数，提取要素并缩短执行时间。\n",
    "\n",
    "通常使用max-pooling或average-pooling。\n",
    "\n",
    "由于在此模型中使用了max-pooling，因此我们定义了max-pooling函数，如下所示:\n",
    "\n",
    "\n",
    "- 输入尺寸（W）\n",
    "- 池化大小（K）\n",
    "- 池化步长（S）\n",
    "- 池化零填充（P）\n",
    "\n",
    "\n",
    "$$ \\frac{W-K+2P}{S}+1 $$\n",
    "\n",
    "\n",
    "在我们的模型中，输入是28x28，池大小是2x2，补长是2，零填充，所以我们将输出大小如下。\n",
    "\n",
    "$$ \\frac{28-2+2\\times0}{2}+1=14 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool(inputs, ksize, stride, padding='VALID', scope_name='pool'):\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "        pool = tf.nn.max_pool(inputs,\n",
    "                            ksize=[1, ksize, ksize, 1],\n",
    "                            strides=[1, stride, stride, 1],\n",
    "                            padding=padding)\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(inputs, out_dim, scope_name='fc'):\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "        in_dim = inputs.shape[-1]\n",
    "        w = tf.get_variable('weights', [in_dim, out_dim],\n",
    "                            initializer=tf.truncated_normal_initializer())\n",
    "        b = tf.get_variable('biases', [out_dim],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        out = tf.matmul(inputs, w) + b\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.组合调用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在让我们通过组合我们创建的函数来创建整个模型。您可以使用我们按顺序创建的功能。\n",
    "\n",
    "需要注意的一点是，当您在最后一次池化后转到fc层时，必须通过将一维向量的大小乘以原始数组的每个维度的长度来重新整形三维数组的一维数组。\n",
    "\n",
    "最后，将dropout应用到fc层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(self):\n",
    "        conv1 = conv_relu(inputs=self.img,\n",
    "                        filters=32,\n",
    "                        k_size=5,\n",
    "                        stride=1,\n",
    "                        padding='SAME',\n",
    "                        scope_name='conv1')\n",
    "        pool1 = maxpool(conv1, 2, 2, 'VALID', 'pool1')\n",
    "        conv2 = conv_relu(inputs=pool1,\n",
    "                        filters=64,\n",
    "                        k_size=5,\n",
    "                        stride=1,\n",
    "                        padding='SAME',\n",
    "                        scope_name='conv2')\n",
    "        pool2 = maxpool(conv2, 2, 2, 'VALID', 'pool2')\n",
    "        feature_dim = pool2.shape[1] * pool2.shape[2] * pool2.shape[3]\n",
    "        pool2 = tf.reshape(pool2, [-1, feature_dim])\n",
    "        fc = tf.nn.relu(fully_connected(pool2, 1024, 'fc'))\n",
    "        dropout = tf.layers.dropout(fc, self.keep_prob, training=self.training, name='dropout')\n",
    "\n",
    "        self.logits = fully_connected(dropout, self.n_classes, 'logits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self):\n",
    "        '''\n",
    "        define loss function\n",
    "        use softmax cross entropy with logits as the loss function\n",
    "        compute mean cross entropy, softmax is applied internally\n",
    "        '''\n",
    "        # \n",
    "        with tf.name_scope('loss'):\n",
    "            entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.label, logits=self.logits)\n",
    "            self.loss = tf.reduce_mean(entropy, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练时，需要评估每个epoch的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(self):\n",
    "        '''\n",
    "        Count the number of right predictions in a batch\n",
    "        '''\n",
    "        with tf.name_scope('predict'):\n",
    "            preds = tf.nn.softmax(self.logits)\n",
    "            correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(self.label, 1))\n",
    "            self.accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/mnist/train-images-idx3-ubyte.gz already exists\n",
      "data/mnist/train-labels-idx1-ubyte.gz already exists\n",
      "data/mnist/t10k-images-idx3-ubyte.gz already exists\n",
      "data/mnist/t10k-labels-idx1-ubyte.gz already exists\n",
      "conv:Tensor(\"conv1_1:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "pool1:Tensor(\"pool1/MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
      "conv2:Tensor(\"conv2_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
      "pool2:Tensor(\"pool2/MaxPool:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
      "feature_dim:3136\n",
      "pool2:Tensor(\"Reshape:0\", shape=(?, 3136), dtype=float32)\n",
      "fc:Tensor(\"fc/add:0\", shape=(?, 1024), dtype=float32)\n",
      "dropout:Tensor(\"relu_dropout/mul:0\", shape=(?, 1024), dtype=float32)\n",
      "self.logits:Tensor(\"logits/add:0\", shape=(?, 10), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-4-c368db8c2aab>:67: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "INFO:tensorflow:Summary name histogram loss is illegal; using histogram_loss instead.\n",
      "Loss at step 19: 15894.556640625\n",
      "Loss at step 39: 8952.953125\n",
      "Loss at step 59: 6065.05322265625\n",
      "Loss at step 79: 2913.25048828125\n",
      "Loss at step 99: 2803.952392578125\n",
      "Loss at step 119: 1727.0462646484375\n",
      "Loss at step 139: 2886.213134765625\n",
      "Loss at step 159: 2611.1953125\n",
      "Loss at step 179: 1743.4693603515625\n",
      "Loss at step 199: 898.48046875\n",
      "Loss at step 219: 2171.2890625\n",
      "Loss at step 239: 475.59246826171875\n",
      "Loss at step 259: 1289.218017578125\n",
      "Loss at step 279: 933.6298828125\n",
      "Loss at step 299: 614.7198486328125\n",
      "Loss at step 319: 1771.800048828125\n",
      "Loss at step 339: 1211.3431396484375\n",
      "Loss at step 359: 1274.873291015625\n",
      "Loss at step 379: 820.397705078125\n",
      "Loss at step 399: 633.9185791015625\n",
      "Loss at step 419: 830.4837646484375\n",
      "Average loss at epoch 0: 3882.1572788682097\n",
      "Took: 15.50917387008667 seconds\n",
      "Accuracy at epoch 0: 0.9099 \n",
      "Took: 0.8531529903411865 seconds\n",
      "Loss at step 439: 758.7982177734375\n",
      "Loss at step 459: 456.7580871582031\n",
      "Loss at step 479: 910.0094604492188\n",
      "Loss at step 499: 880.6016845703125\n",
      "Loss at step 519: 372.9656982421875\n",
      "Loss at step 539: 645.856201171875\n",
      "Loss at step 559: 557.6192016601562\n",
      "Loss at step 579: 454.9212951660156\n",
      "Loss at step 599: 220.226806640625\n",
      "Loss at step 619: 478.953857421875\n",
      "Loss at step 639: 500.7513732910156\n",
      "Loss at step 659: 381.390869140625\n",
      "Loss at step 679: 571.0089721679688\n",
      "Loss at step 699: 137.5841522216797\n",
      "Loss at step 719: 581.2334594726562\n",
      "Loss at step 739: 444.5173034667969\n",
      "Loss at step 759: 87.88299560546875\n",
      "Loss at step 779: 240.0809326171875\n",
      "Loss at step 799: 573.7052612304688\n",
      "Loss at step 819: 287.5755920410156\n",
      "Loss at step 839: 257.1443786621094\n",
      "Loss at step 859: 409.34375\n",
      "Average loss at epoch 1: 488.3693239256393\n",
      "Took: 15.131999731063843 seconds\n",
      "Accuracy at epoch 1: 0.9391 \n",
      "Took: 0.755587100982666 seconds\n",
      "Loss at step 879: 151.05177307128906\n",
      "Loss at step 899: 257.8609619140625\n",
      "Loss at step 919: 329.6173095703125\n",
      "Loss at step 939: 284.8762512207031\n",
      "Loss at step 959: 384.9897155761719\n",
      "Loss at step 979: 0.0\n",
      "Loss at step 999: 315.7909240722656\n",
      "Loss at step 1019: 381.55279541015625\n",
      "Loss at step 1039: 251.58531188964844\n",
      "Loss at step 1059: 219.8668975830078\n",
      "Loss at step 1079: 196.7032470703125\n",
      "Loss at step 1099: 88.50685119628906\n",
      "Loss at step 1119: 382.7196044921875\n",
      "Loss at step 1139: 224.0420379638672\n",
      "Loss at step 1159: 252.48570251464844\n",
      "Loss at step 1179: 247.4862060546875\n",
      "Loss at step 1199: 263.6212463378906\n",
      "Loss at step 1219: 288.5655517578125\n",
      "Loss at step 1239: 243.6124267578125\n",
      "Loss at step 1259: 274.0865783691406\n",
      "Loss at step 1279: 112.85366821289062\n",
      "Average loss at epoch 2: 249.8322339878526\n",
      "Took: 14.13972282409668 seconds\n",
      "Accuracy at epoch 2: 0.9544 \n",
      "Took: 0.9117927551269531 seconds\n",
      "Loss at step 1299: 155.59837341308594\n",
      "Loss at step 1319: 153.76309204101562\n",
      "Loss at step 1339: 142.55465698242188\n",
      "Loss at step 1359: 77.88518524169922\n",
      "Loss at step 1379: 62.282623291015625\n",
      "Loss at step 1399: 125.25086975097656\n",
      "Loss at step 1419: 125.84466552734375\n",
      "Loss at step 1439: 36.85096740722656\n",
      "Loss at step 1459: 53.37446594238281\n",
      "Loss at step 1479: 292.78900146484375\n",
      "Loss at step 1499: 167.0472412109375\n",
      "Loss at step 1519: 234.7626190185547\n",
      "Loss at step 1539: 260.38525390625\n",
      "Loss at step 1559: 75.6225357055664\n",
      "Loss at step 1579: 173.0400390625\n",
      "Loss at step 1599: 142.8729248046875\n",
      "Loss at step 1619: 511.7841796875\n",
      "Loss at step 1639: 238.49984741210938\n",
      "Loss at step 1659: 199.40130615234375\n",
      "Loss at step 1679: 77.54933166503906\n",
      "Loss at step 1699: 106.59364318847656\n",
      "Loss at step 1719: 98.59701538085938\n",
      "Average loss at epoch 3: 159.70596207019895\n",
      "Took: 14.144736289978027 seconds\n",
      "Accuracy at epoch 3: 0.9596 \n",
      "Took: 0.7955155372619629 seconds\n",
      "Loss at step 1739: 126.69613647460938\n",
      "Loss at step 1759: 53.659263610839844\n",
      "Loss at step 1779: 152.20755004882812\n",
      "Loss at step 1799: 241.73306274414062\n",
      "Loss at step 1819: 132.12286376953125\n",
      "Loss at step 1839: 27.341060638427734\n",
      "Loss at step 1859: 38.45719909667969\n",
      "Loss at step 1879: 140.38983154296875\n",
      "Loss at step 1899: 146.14053344726562\n",
      "Loss at step 1919: 26.470333099365234\n",
      "Loss at step 1939: 159.908447265625\n",
      "Loss at step 1959: 101.10511016845703\n",
      "Loss at step 1979: 102.38552856445312\n",
      "Loss at step 1999: 132.39625549316406\n",
      "Loss at step 2019: 49.47382354736328\n",
      "Loss at step 2039: 129.1416778564453\n",
      "Loss at step 2059: 5.2291412353515625\n",
      "Loss at step 2079: 104.48294067382812\n",
      "Loss at step 2099: 37.64787292480469\n",
      "Loss at step 2119: 58.094017028808594\n",
      "Loss at step 2139: 126.61505889892578\n",
      "Average loss at epoch 4: 107.4441191917242\n",
      "Took: 14.383376598358154 seconds\n",
      "Accuracy at epoch 4: 0.9611 \n",
      "Took: 0.7973875999450684 seconds\n",
      "Loss at step 2159: 113.36485290527344\n",
      "Loss at step 2179: 84.62918853759766\n",
      "Loss at step 2199: 132.22662353515625\n",
      "Loss at step 2219: 122.1037368774414\n",
      "Loss at step 2239: 108.21060180664062\n",
      "Loss at step 2259: 115.04078674316406\n",
      "Loss at step 2279: 27.676353454589844\n",
      "Loss at step 2299: 62.546241760253906\n",
      "Loss at step 2319: 103.34405517578125\n",
      "Loss at step 2339: 0.0\n",
      "Loss at step 2359: 106.5550765991211\n",
      "Loss at step 2379: 49.957191467285156\n",
      "Loss at step 2399: 64.8963394165039\n",
      "Loss at step 2419: 122.23128509521484\n",
      "Loss at step 2439: 7.87469482421875\n",
      "Loss at step 2459: 55.149349212646484\n",
      "Loss at step 2479: 71.72201538085938\n",
      "Loss at step 2499: 21.220491409301758\n",
      "Loss at step 2519: 63.84972381591797\n",
      "Loss at step 2539: 53.08820343017578\n",
      "Loss at step 2559: 90.94804382324219\n",
      "Loss at step 2579: 107.3580093383789\n",
      "Average loss at epoch 5: 75.73444628161053\n",
      "Took: 14.106810331344604 seconds\n",
      "Accuracy at epoch 5: 0.9658 \n",
      "Took: 0.835106611251831 seconds\n",
      "Loss at step 2599: 73.33642578125\n",
      "Loss at step 2619: 118.55120086669922\n",
      "Loss at step 2639: 131.9439697265625\n",
      "Loss at step 2659: 51.76431655883789\n",
      "Loss at step 2679: 9.0551176071167\n",
      "Loss at step 2699: 117.07209014892578\n",
      "Loss at step 2719: 72.59506225585938\n",
      "Loss at step 2739: 82.38190460205078\n",
      "Loss at step 2759: 30.725448608398438\n",
      "Loss at step 2779: 26.50537872314453\n",
      "Loss at step 2799: 40.071861267089844\n",
      "Loss at step 2819: 56.682891845703125\n",
      "Loss at step 2839: 0.0\n",
      "Loss at step 2859: 80.55841064453125\n",
      "Loss at step 2879: 46.77141189575195\n",
      "Loss at step 2899: 19.21314811706543\n",
      "Loss at step 2919: 58.30522537231445\n",
      "Loss at step 2939: 21.982410430908203\n",
      "Loss at step 2959: 39.78470993041992\n",
      "Loss at step 2979: 37.60313034057617\n",
      "Loss at step 2999: 9.140384674072266\n",
      "Average loss at epoch 6: 54.462093258458516\n",
      "Took: 14.077213764190674 seconds\n",
      "Accuracy at epoch 6: 0.9671 \n",
      "Took: 0.8674428462982178 seconds\n",
      "Loss at step 3019: 2.5832290649414062\n",
      "Loss at step 3039: 27.76649284362793\n",
      "Loss at step 3059: 0.0\n",
      "Loss at step 3079: 107.02574157714844\n",
      "Loss at step 3099: 26.76766586303711\n",
      "Loss at step 3119: 55.25819396972656\n",
      "Loss at step 3139: 12.419807434082031\n",
      "Loss at step 3159: 34.99305725097656\n",
      "Loss at step 3179: 25.025196075439453\n",
      "Loss at step 3199: 30.320186614990234\n",
      "Loss at step 3219: 13.118598937988281\n",
      "Loss at step 3239: 70.06871032714844\n",
      "Loss at step 3259: 36.468650817871094\n",
      "Loss at step 3279: 2.245319366455078\n",
      "Loss at step 3299: 66.77931213378906\n",
      "Loss at step 3319: 6.985343933105469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 3339: 32.812469482421875\n",
      "Loss at step 3359: 9.880901336669922\n",
      "Loss at step 3379: 37.180015563964844\n",
      "Loss at step 3399: 20.444835662841797\n",
      "Loss at step 3419: 25.90365219116211\n",
      "Loss at step 3439: 11.969549179077148\n",
      "Average loss at epoch 7: 42.81812903160272\n",
      "Took: 15.069268465042114 seconds\n",
      "Accuracy at epoch 7: 0.9686 \n",
      "Took: 0.8306162357330322 seconds\n",
      "Loss at step 3459: 102.39554595947266\n",
      "Loss at step 3479: 46.702369689941406\n",
      "Loss at step 3499: 0.0\n",
      "Loss at step 3519: 27.893085479736328\n",
      "Loss at step 3539: 4.3278045654296875\n",
      "Loss at step 3559: 4.8789825439453125\n",
      "Loss at step 3579: 44.614688873291016\n",
      "Loss at step 3599: 0.0\n",
      "Loss at step 3619: 95.66297912597656\n",
      "Loss at step 3639: 41.93891906738281\n",
      "Loss at step 3659: 81.085205078125\n",
      "Loss at step 3679: 38.55162048339844\n",
      "Loss at step 3699: 28.404909133911133\n",
      "Loss at step 3719: 40.942832946777344\n",
      "Loss at step 3739: 48.79941177368164\n",
      "Loss at step 3759: 44.761146545410156\n",
      "Loss at step 3779: 21.152957916259766\n",
      "Loss at step 3799: 24.45763397216797\n",
      "Loss at step 3819: 96.14579772949219\n",
      "Loss at step 3839: 18.216157913208008\n",
      "Loss at step 3859: 13.902820587158203\n",
      "Average loss at epoch 8: 34.71453904903692\n",
      "Took: 14.50889277458191 seconds\n",
      "Accuracy at epoch 8: 0.9694 \n",
      "Took: 0.8126306533813477 seconds\n",
      "Loss at step 3879: 56.56782913208008\n",
      "Loss at step 3899: 2.036052703857422\n",
      "Loss at step 3919: 34.639854431152344\n",
      "Loss at step 3939: 9.088821411132812\n",
      "Loss at step 3959: 0.0\n",
      "Loss at step 3979: 0.0\n",
      "Loss at step 3999: 35.599369049072266\n",
      "Loss at step 4019: 59.12310791015625\n",
      "Loss at step 4039: 60.17096710205078\n",
      "Loss at step 4059: 1.7484130859375\n",
      "Loss at step 4079: 0.0\n",
      "Loss at step 4099: 9.125885009765625\n",
      "Loss at step 4119: 5.872539520263672\n",
      "Loss at step 4139: 3.6303634643554688\n",
      "Loss at step 4159: 11.877674102783203\n",
      "Loss at step 4179: 0.59979248046875\n",
      "Loss at step 4199: 10.723243713378906\n",
      "Loss at step 4219: 11.294681549072266\n",
      "Loss at step 4239: 44.99151611328125\n",
      "Loss at step 4259: 5.205234527587891\n",
      "Loss at step 4279: 51.480812072753906\n",
      "Loss at step 4299: 0.0\n",
      "Average loss at epoch 9: 28.96083330434942\n",
      "Took: 14.242049217224121 seconds\n",
      "Accuracy at epoch 9: 0.9706 \n",
      "Took: 0.8100645542144775 seconds\n",
      "Loss at step 4319: 21.482635498046875\n",
      "Loss at step 4339: 8.6820068359375\n",
      "Loss at step 4359: 13.973161697387695\n",
      "Loss at step 4379: 0.0\n",
      "Loss at step 4399: 0.0\n",
      "Loss at step 4419: 0.0\n",
      "Loss at step 4439: 22.924686431884766\n",
      "Loss at step 4459: 0.392578125\n",
      "Loss at step 4479: 17.72028350830078\n",
      "Loss at step 4499: 0.0\n",
      "Loss at step 4519: 14.22736930847168\n",
      "Loss at step 4539: 73.652587890625\n",
      "Loss at step 4559: 12.380439758300781\n",
      "Loss at step 4579: 54.44392013549805\n",
      "Loss at step 4599: 60.60076141357422\n",
      "Loss at step 4619: 8.967906951904297\n",
      "Loss at step 4639: 0.0\n",
      "Loss at step 4659: 4.5587310791015625\n",
      "Loss at step 4679: 32.90028762817383\n",
      "Loss at step 4699: 51.79493713378906\n",
      "Loss at step 4719: 11.239540100097656\n",
      "Average loss at epoch 10: 24.5784894758528\n",
      "Took: 14.923862934112549 seconds\n",
      "Accuracy at epoch 10: 0.9722 \n",
      "Took: 0.7770974636077881 seconds\n",
      "Loss at step 4739: 69.14229583740234\n",
      "Loss at step 4759: 0.0\n",
      "Loss at step 4779: 12.058473587036133\n",
      "Loss at step 4799: 4.182018280029297\n",
      "Loss at step 4819: 56.867191314697266\n",
      "Loss at step 4839: 16.628833770751953\n",
      "Loss at step 4859: 9.625240325927734\n",
      "Loss at step 4879: 0.0\n",
      "Loss at step 4899: 20.2486629486084\n",
      "Loss at step 4919: 3.1170921325683594\n",
      "Loss at step 4939: 39.58266830444336\n",
      "Loss at step 4959: 11.792362213134766\n",
      "Loss at step 4979: 9.121246337890625\n",
      "Loss at step 4999: 6.3537750244140625\n",
      "Loss at step 5019: 11.788719177246094\n",
      "Loss at step 5039: 20.47061538696289\n",
      "Loss at step 5059: 3.493499755859375\n",
      "Loss at step 5079: 34.29701232910156\n",
      "Loss at step 5099: 6.589927673339844\n",
      "Loss at step 5119: 59.27741241455078\n",
      "Loss at step 5139: 12.234540939331055\n",
      "Loss at step 5159: 0.0\n",
      "Average loss at epoch 11: 20.19462705675599\n",
      "Took: 14.405394554138184 seconds\n",
      "Accuracy at epoch 11: 0.9737 \n",
      "Took: 0.8390228748321533 seconds\n",
      "Loss at step 5179: 55.864585876464844\n",
      "Loss at step 5199: 5.9595794677734375\n",
      "Loss at step 5219: 17.728673934936523\n",
      "Loss at step 5239: 5.807682037353516\n",
      "Loss at step 5259: 12.842254638671875\n",
      "Loss at step 5279: 0.0\n",
      "Loss at step 5299: 4.866096496582031\n",
      "Loss at step 5319: 7.6767578125\n",
      "Loss at step 5339: 9.40202522277832\n",
      "Loss at step 5359: 0.43334197998046875\n",
      "Loss at step 5379: 18.64937973022461\n",
      "Loss at step 5399: 73.58853149414062\n",
      "Loss at step 5419: 13.251659393310547\n",
      "Loss at step 5439: 0.0\n",
      "Loss at step 5459: 1.5840530395507812\n",
      "Loss at step 5479: 1.0236053466796875\n",
      "Loss at step 5499: 0.0\n",
      "Loss at step 5519: 0.0\n",
      "Loss at step 5539: 57.453216552734375\n",
      "Loss at step 5559: 0.0\n",
      "Loss at step 5579: 6.737331390380859\n",
      "Average loss at epoch 12: 16.55802519336386\n",
      "Took: 14.268776655197144 seconds\n",
      "Accuracy at epoch 12: 0.974 \n",
      "Took: 0.7592968940734863 seconds\n",
      "Loss at step 5599: 8.079843521118164\n",
      "Loss at step 5619: 0.0\n",
      "Loss at step 5639: 15.677238464355469\n",
      "Loss at step 5659: 25.877843856811523\n",
      "Loss at step 5679: 11.573921203613281\n",
      "Loss at step 5699: 10.502168655395508\n",
      "Loss at step 5719: 20.491302490234375\n",
      "Loss at step 5739: 22.888477325439453\n",
      "Loss at step 5759: 6.082372665405273\n",
      "Loss at step 5779: 18.14913558959961\n",
      "Loss at step 5799: 3.6617088317871094\n",
      "Loss at step 5819: 0.0\n",
      "Loss at step 5839: 36.47123718261719\n",
      "Loss at step 5859: 0.0\n",
      "Loss at step 5879: 44.48242950439453\n",
      "Loss at step 5899: 0.0\n",
      "Loss at step 5919: 0.0\n",
      "Loss at step 5939: 33.51776123046875\n",
      "Loss at step 5959: 17.182537078857422\n",
      "Loss at step 5979: 12.121330261230469\n",
      "Loss at step 5999: 0.47746849060058594\n",
      "Loss at step 6019: 0.0\n",
      "Average loss at epoch 13: 15.274544461940701\n",
      "Took: 14.650165557861328 seconds\n",
      "Accuracy at epoch 13: 0.9756 \n",
      "Took: 0.8857109546661377 seconds\n",
      "Loss at step 6039: 0.0\n",
      "Loss at step 6059: 0.0\n",
      "Loss at step 6079: 0.0\n",
      "Loss at step 6099: 6.519538879394531\n",
      "Loss at step 6119: 0.0\n",
      "Loss at step 6139: 11.615776062011719\n",
      "Loss at step 6159: 16.6630802154541\n",
      "Loss at step 6179: 29.52825927734375\n",
      "Loss at step 6199: 38.11650085449219\n",
      "Loss at step 6219: 15.82614517211914\n",
      "Loss at step 6239: 0.0\n",
      "Loss at step 6259: 13.358718872070312\n",
      "Loss at step 6279: 11.423614501953125\n",
      "Loss at step 6299: 15.111927032470703\n",
      "Loss at step 6319: 14.53622055053711\n",
      "Loss at step 6339: 16.08694076538086\n",
      "Loss at step 6359: 13.97874927520752\n",
      "Loss at step 6379: 15.091020584106445\n",
      "Loss at step 6399: 0.0\n",
      "Loss at step 6419: 34.05900955200195\n",
      "Loss at step 6439: 0.0\n",
      "Average loss at epoch 14: 13.2105347957815\n",
      "Took: 13.504944562911987 seconds\n",
      "Accuracy at epoch 14: 0.9763 \n",
      "Took: 0.7477085590362549 seconds\n",
      "Loss at step 6459: 7.366788864135742\n",
      "Loss at step 6479: 24.889951705932617\n",
      "Loss at step 6499: 8.90946102142334\n",
      "Loss at step 6519: 0.0\n",
      "Loss at step 6539: 1.5747737884521484\n",
      "Loss at step 6559: 24.676647186279297\n",
      "Loss at step 6579: 0.0\n",
      "Loss at step 6599: 4.097530364990234\n",
      "Loss at step 6619: 3.655353546142578\n",
      "Loss at step 6639: 6.970489501953125\n",
      "Loss at step 6659: 4.824188232421875\n",
      "Loss at step 6679: 8.82666015625\n",
      "Loss at step 6699: 14.861251831054688\n",
      "Loss at step 6719: 2.543121337890625\n",
      "Loss at step 6739: 7.8005828857421875\n",
      "Loss at step 6759: 40.692054748535156\n",
      "Loss at step 6779: 32.777488708496094\n",
      "Loss at step 6799: 14.999458312988281\n",
      "Loss at step 6819: 0.0\n",
      "Loss at step 6839: 12.979240417480469\n",
      "Loss at step 6859: 0.0\n",
      "Loss at step 6879: 13.779906272888184\n",
      "Average loss at epoch 15: 12.322203885386378\n",
      "Took: 14.124963521957397 seconds\n",
      "Accuracy at epoch 15: 0.9768 \n",
      "Took: 0.7491662502288818 seconds\n",
      "Loss at step 6899: 0.0\n",
      "Loss at step 6919: 0.0\n",
      "Loss at step 6939: 29.60796356201172\n",
      "Loss at step 6959: 2.6179771423339844\n",
      "Loss at step 6979: 25.00567626953125\n",
      "Loss at step 6999: 18.94452667236328\n",
      "Loss at step 7019: 8.524641036987305\n",
      "Loss at step 7039: 10.529090881347656\n",
      "Loss at step 7059: 12.72507095336914\n",
      "Loss at step 7079: 0.0\n",
      "Loss at step 7099: 9.991485595703125\n",
      "Loss at step 7119: 20.858184814453125\n",
      "Loss at step 7139: 22.738319396972656\n",
      "Loss at step 7159: 2.0843124389648438\n",
      "Loss at step 7179: 0.0\n",
      "Loss at step 7199: 21.10162353515625\n",
      "Loss at step 7219: 2.9306640625\n",
      "Loss at step 7239: 23.108867645263672\n",
      "Loss at step 7259: 0.0\n",
      "Loss at step 7279: 0.0\n",
      "Loss at step 7299: 27.912813186645508\n",
      "Average loss at epoch 16: 10.09059307425781\n",
      "Took: 13.521292686462402 seconds\n",
      "Accuracy at epoch 16: 0.978 \n",
      "Took: 0.7468390464782715 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 7319: 2.5546417236328125\n",
      "Loss at step 7339: 12.190032958984375\n",
      "Loss at step 7359: 24.120805740356445\n",
      "Loss at step 7379: 7.202362060546875\n",
      "Loss at step 7399: 0.0\n",
      "Loss at step 7419: 0.0\n",
      "Loss at step 7439: 0.0\n",
      "Loss at step 7459: 0.0\n",
      "Loss at step 7479: 17.14218521118164\n",
      "Loss at step 7499: 0.0\n",
      "Loss at step 7519: 0.0\n",
      "Loss at step 7539: 4.968559265136719\n",
      "Loss at step 7559: 2.9908065795898438\n",
      "Loss at step 7579: 1.8168716430664062\n",
      "Loss at step 7599: 3.7762832641601562\n",
      "Loss at step 7619: 31.4625301361084\n",
      "Loss at step 7639: 0.0\n",
      "Loss at step 7659: 14.590621948242188\n",
      "Loss at step 7679: 12.20016098022461\n",
      "Loss at step 7699: 0.0\n",
      "Loss at step 7719: 0.0\n",
      "Loss at step 7739: 0.0\n",
      "Average loss at epoch 17: 8.916982574495918\n",
      "Took: 13.504043102264404 seconds\n",
      "Accuracy at epoch 17: 0.9785 \n",
      "Took: 0.7479231357574463 seconds\n",
      "Loss at step 7759: 23.192916870117188\n",
      "Loss at step 7779: 18.947620391845703\n",
      "Loss at step 7799: 0.0\n",
      "Loss at step 7819: 14.19647216796875\n",
      "Loss at step 7839: 16.239910125732422\n",
      "Loss at step 7859: 13.603870391845703\n",
      "Loss at step 7879: 0.0\n",
      "Loss at step 7899: 0.0\n",
      "Loss at step 7919: 0.0\n",
      "Loss at step 7939: 18.78318214416504\n",
      "Loss at step 7959: 10.085710525512695\n",
      "Loss at step 7979: 0.0\n",
      "Loss at step 7999: 7.213470458984375\n",
      "Loss at step 8019: 0.5740203857421875\n",
      "Loss at step 8039: 3.3799514770507812\n",
      "Loss at step 8059: 3.4420604705810547\n",
      "Loss at step 8079: 12.473976135253906\n",
      "Loss at step 8099: 16.580970764160156\n",
      "Loss at step 8119: 0.0\n",
      "Loss at step 8139: 0.0\n",
      "Loss at step 8159: 5.960700988769531\n",
      "Average loss at epoch 18: 8.43590178261105\n",
      "Took: 13.503224849700928 seconds\n",
      "Accuracy at epoch 18: 0.9772 \n",
      "Took: 0.747689962387085 seconds\n",
      "Loss at step 8179: 7.043964385986328\n",
      "Loss at step 8199: 16.92465591430664\n",
      "Loss at step 8219: 4.5381927490234375\n",
      "Loss at step 8239: 14.08791732788086\n",
      "Loss at step 8259: 0.0\n",
      "Loss at step 8279: 0.0\n",
      "Loss at step 8299: 1.0234564342681551e-06\n",
      "Loss at step 8319: 0.0\n",
      "Loss at step 8339: 12.81014633178711\n",
      "Loss at step 8359: 0.19313812255859375\n",
      "Loss at step 8379: 0.0\n",
      "Loss at step 8399: 0.0\n",
      "Loss at step 8419: 14.924972534179688\n",
      "Loss at step 8439: 3.0790634155273438\n",
      "Loss at step 8459: 38.12117385864258\n",
      "Loss at step 8479: 9.930139541625977\n",
      "Loss at step 8499: 5.1520538330078125\n",
      "Loss at step 8519: 0.0\n",
      "Loss at step 8539: 1.8241920471191406\n",
      "Loss at step 8559: 20.86663055419922\n",
      "Loss at step 8579: 0.09485630691051483\n",
      "Loss at step 8599: 0.0\n",
      "Average loss at epoch 19: 7.8902905206296925\n",
      "Took: 14.567058324813843 seconds\n",
      "Accuracy at epoch 19: 0.9799 \n",
      "Took: 0.7718930244445801 seconds\n",
      "Loss at step 8619: 0.1845855712890625\n",
      "Loss at step 8639: 22.205589294433594\n",
      "Loss at step 8659: 0.0\n",
      "Loss at step 8679: 0.0\n",
      "Loss at step 8699: 3.204906463623047\n",
      "Loss at step 8719: 10.200267791748047\n",
      "Loss at step 8739: 12.568672180175781\n",
      "Loss at step 8759: 0.0\n",
      "Loss at step 8779: 0.0\n",
      "Loss at step 8799: 0.0\n",
      "Loss at step 8819: 0.7498159408569336\n",
      "Loss at step 8839: 0.0\n",
      "Loss at step 8859: 0.0\n",
      "Loss at step 8879: 2.7312240600585938\n",
      "Loss at step 8899: 2.7163162231445312\n",
      "Loss at step 8919: 7.750419616699219\n",
      "Loss at step 8939: 18.35006332397461\n",
      "Loss at step 8959: 0.0\n",
      "Loss at step 8979: 0.0\n",
      "Loss at step 8999: 6.942317962646484\n",
      "Loss at step 9019: 17.587932586669922\n",
      "Average loss at epoch 20: 6.91916788882587\n",
      "Took: 13.502917528152466 seconds\n",
      "Accuracy at epoch 20: 0.978 \n",
      "Took: 0.7460079193115234 seconds\n",
      "Loss at step 9039: 0.0\n",
      "Loss at step 9059: 31.040082931518555\n",
      "Loss at step 9079: 0.0\n",
      "Loss at step 9099: 7.607416152954102\n",
      "Loss at step 9119: 11.418121337890625\n",
      "Loss at step 9139: 0.0\n",
      "Loss at step 9159: 3.3678932189941406\n",
      "Loss at step 9179: 0.21695327758789062\n",
      "Loss at step 9199: 0.0\n",
      "Loss at step 9219: 5.336503982543945\n",
      "Loss at step 9239: 13.717643737792969\n",
      "Loss at step 9259: 0.0\n",
      "Loss at step 9279: 0.0\n",
      "Loss at step 9299: 0.0\n",
      "Loss at step 9319: 0.0\n",
      "Loss at step 9339: 5.320037841796875\n",
      "Loss at step 9359: 0.0\n",
      "Loss at step 9379: 1.120513916015625\n",
      "Loss at step 9399: 0.0\n",
      "Loss at step 9419: 7.31260871887207\n",
      "Loss at step 9439: 0.0\n",
      "Loss at step 9459: 24.60381507873535\n",
      "Average loss at epoch 21: 7.033162275348337\n",
      "Took: 13.501884460449219 seconds\n",
      "Accuracy at epoch 21: 0.9776 \n",
      "Took: 0.7514691352844238 seconds\n",
      "Loss at step 9479: 0.0\n",
      "Loss at step 9499: 0.0\n",
      "Loss at step 9519: 7.2837677001953125\n",
      "Loss at step 9539: 8.425308227539062\n",
      "Loss at step 9559: 12.260347366333008\n",
      "Loss at step 9579: 0.0\n",
      "Loss at step 9599: 15.59170913696289\n",
      "Loss at step 9619: 0.0\n",
      "Loss at step 9639: 0.0\n",
      "Loss at step 9659: 11.95418930053711\n",
      "Loss at step 9679: 13.068115234375\n",
      "Loss at step 9699: 0.7838973999023438\n",
      "Loss at step 9719: 17.451444625854492\n",
      "Loss at step 9739: 9.513748168945312\n",
      "Loss at step 9759: 2.7044496536254883\n",
      "Loss at step 9779: 0.0\n",
      "Loss at step 9799: 0.0\n",
      "Loss at step 9819: 0.0\n",
      "Loss at step 9839: 0.0\n",
      "Loss at step 9859: 0.0\n",
      "Loss at step 9879: 0.0\n",
      "Average loss at epoch 22: 5.970003503478074\n",
      "Took: 13.502354860305786 seconds\n",
      "Accuracy at epoch 22: 0.9811 \n",
      "Took: 0.7491979598999023 seconds\n",
      "Loss at step 9899: 0.0\n",
      "Loss at step 9919: 10.6888427734375\n",
      "Loss at step 9939: 9.039268493652344\n",
      "Loss at step 9959: 2.872722625732422\n",
      "Loss at step 9979: 3.153697967529297\n",
      "Loss at step 9999: 3.8206100463867188\n",
      "Loss at step 10019: 2.4813995361328125\n",
      "Loss at step 10039: 0.0\n",
      "Loss at step 10059: 0.26853179931640625\n",
      "Loss at step 10079: 0.0\n",
      "Loss at step 10099: 6.663156509399414\n",
      "Loss at step 10119: 1.3650226593017578\n",
      "Loss at step 10139: 0.0\n",
      "Loss at step 10159: 10.00787353515625\n",
      "Loss at step 10179: 0.0\n",
      "Loss at step 10199: 1.836324691772461\n",
      "Loss at step 10219: 6.451162338256836\n",
      "Loss at step 10239: 0.0\n",
      "Loss at step 10259: 0.0\n",
      "Loss at step 10279: 13.731071472167969\n",
      "Loss at step 10299: 23.159770965576172\n",
      "Loss at step 10319: 0.0\n",
      "Average loss at epoch 23: 6.26864860184534\n",
      "Took: 13.509432792663574 seconds\n",
      "Accuracy at epoch 23: 0.9808 \n",
      "Took: 0.751422643661499 seconds\n",
      "Loss at step 10339: 0.0\n",
      "Loss at step 10359: 39.019317626953125\n",
      "Loss at step 10379: 0.0\n",
      "Loss at step 10399: 0.0\n",
      "Loss at step 10419: 6.701982498168945\n",
      "Loss at step 10439: 0.26744651794433594\n",
      "Loss at step 10459: 2.7149276733398438\n",
      "Loss at step 10479: 0.0\n",
      "Loss at step 10499: 3.6681861877441406\n",
      "Loss at step 10519: 0.0\n",
      "Loss at step 10539: 0.0\n",
      "Loss at step 10559: 14.437206268310547\n",
      "Loss at step 10579: 0.0\n",
      "Loss at step 10599: 0.0\n",
      "Loss at step 10619: 2.6719589233398438\n",
      "Loss at step 10639: 44.94112777709961\n",
      "Loss at step 10659: 0.0\n",
      "Loss at step 10679: 0.0\n",
      "Loss at step 10699: 8.454517364501953\n",
      "Loss at step 10719: 7.001277923583984\n",
      "Loss at step 10739: 0.0\n",
      "Average loss at epoch 24: 4.80496865536246\n",
      "Took: 13.520602464675903 seconds\n",
      "Accuracy at epoch 24: 0.9811 \n",
      "Took: 0.7509057521820068 seconds\n",
      "Loss at step 10759: 0.0\n",
      "Loss at step 10779: 1.4485664367675781\n",
      "Loss at step 10799: 0.0\n",
      "Loss at step 10819: 0.9312095642089844\n",
      "Loss at step 10839: 12.055294036865234\n",
      "Loss at step 10859: 8.439424514770508\n",
      "Loss at step 10879: 0.0\n",
      "Loss at step 10899: 6.769382476806641\n",
      "Loss at step 10919: 9.289321899414062\n",
      "Loss at step 10939: 12.339408874511719\n",
      "Loss at step 10959: 0.0\n",
      "Loss at step 10979: 0.0\n",
      "Loss at step 10999: 0.0\n",
      "Loss at step 11019: 0.0\n",
      "Loss at step 11039: 1.8080253601074219\n",
      "Loss at step 11059: 3.6224746704101562\n",
      "Loss at step 11079: 0.0\n",
      "Loss at step 11099: 0.0\n",
      "Loss at step 11119: 0.0\n",
      "Loss at step 11139: 0.0\n",
      "Loss at step 11159: 0.0\n",
      "Loss at step 11179: 0.0\n",
      "Average loss at epoch 25: 5.264740123009839\n",
      "Took: 14.476869821548462 seconds\n",
      "Accuracy at epoch 25: 0.983 \n",
      "Took: 0.7487013339996338 seconds\n",
      "Loss at step 11199: 20.94754409790039\n",
      "Loss at step 11219: 0.0\n",
      "Loss at step 11239: 21.948535919189453\n",
      "Loss at step 11259: 0.0\n",
      "Loss at step 11279: 0.0\n",
      "Loss at step 11299: 20.713054656982422\n",
      "Loss at step 11319: 27.30608367919922\n",
      "Loss at step 11339: 0.0\n",
      "Loss at step 11359: 1.4736137390136719\n",
      "Loss at step 11379: 2.116607666015625\n",
      "Loss at step 11399: 0.0\n",
      "Loss at step 11419: 3.1991119384765625\n",
      "Loss at step 11439: 8.44703483581543\n",
      "Loss at step 11459: 0.0\n",
      "Loss at step 11479: 0.0\n",
      "Loss at step 11499: 0.0\n",
      "Loss at step 11519: 1.3609066009521484\n",
      "Loss at step 11539: 0.0\n",
      "Loss at step 11559: 0.0\n",
      "Loss at step 11579: 6.063764572143555\n",
      "Loss at step 11599: 8.876453399658203\n",
      "Average loss at epoch 26: 5.119343305078368\n",
      "Took: 13.510163307189941 seconds\n",
      "Accuracy at epoch 26: 0.9817 \n",
      "Took: 0.7514593601226807 seconds\n",
      "Loss at step 11619: 19.14657211303711\n",
      "Loss at step 11639: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 11659: 0.0\n",
      "Loss at step 11679: 0.0\n",
      "Loss at step 11699: 0.0\n",
      "Loss at step 11719: 14.354545593261719\n",
      "Loss at step 11739: 0.0\n",
      "Loss at step 11759: 0.0\n",
      "Loss at step 11779: 0.0\n",
      "Loss at step 11799: 6.890388488769531\n",
      "Loss at step 11819: 0.0\n",
      "Loss at step 11839: 16.228740692138672\n",
      "Loss at step 11859: 0.0\n",
      "Loss at step 11879: 0.0\n",
      "Loss at step 11899: 0.0\n",
      "Loss at step 11919: 0.0\n",
      "Loss at step 11939: 7.5826873779296875\n",
      "Loss at step 11959: 0.0\n",
      "Loss at step 11979: 0.0\n",
      "Loss at step 11999: 0.0\n",
      "Loss at step 12019: 0.0\n",
      "Loss at step 12039: 0.0\n",
      "Average loss at epoch 27: 4.242783787411226\n",
      "Took: 13.512002944946289 seconds\n",
      "Accuracy at epoch 27: 0.9822 \n",
      "Took: 0.7501888275146484 seconds\n",
      "Loss at step 12059: 0.0\n",
      "Loss at step 12079: 15.846878051757812\n",
      "Loss at step 12099: 0.0\n",
      "Loss at step 12119: 0.0\n",
      "Loss at step 12139: 0.0\n",
      "Loss at step 12159: 0.0\n",
      "Loss at step 12179: 0.0\n",
      "Loss at step 12199: 0.0\n",
      "Loss at step 12219: 5.127838134765625\n",
      "Loss at step 12239: 0.0\n",
      "Loss at step 12259: 8.277763366699219\n",
      "Loss at step 12279: 0.0\n",
      "Loss at step 12299: 0.0\n",
      "Loss at step 12319: 0.0\n",
      "Loss at step 12339: 6.243832111358643\n",
      "Loss at step 12359: 14.181388854980469\n",
      "Loss at step 12379: 2.8281517028808594\n",
      "Loss at step 12399: 0.0\n",
      "Loss at step 12419: 0.0\n",
      "Loss at step 12439: 9.666204452514648\n",
      "Loss at step 12459: 15.324836730957031\n",
      "Average loss at epoch 28: 3.975543207926715\n",
      "Took: 14.37944221496582 seconds\n",
      "Accuracy at epoch 28: 0.9823 \n",
      "Took: 0.7509803771972656 seconds\n",
      "Loss at step 12479: 13.472938537597656\n",
      "Loss at step 12499: 0.0\n",
      "Loss at step 12519: 0.0\n",
      "Loss at step 12539: 0.0\n",
      "Loss at step 12559: 0.0\n",
      "Loss at step 12579: 0.2893028259277344\n",
      "Loss at step 12599: 0.0\n",
      "Loss at step 12619: 0.6415176391601562\n",
      "Loss at step 12639: 4.451316833496094\n",
      "Loss at step 12659: 0.0\n",
      "Loss at step 12679: 0.0\n",
      "Loss at step 12699: 0.0\n",
      "Loss at step 12719: 0.0\n",
      "Loss at step 12739: 3.427783966064453\n",
      "Loss at step 12759: 41.38375473022461\n",
      "Loss at step 12779: 16.73694610595703\n",
      "Loss at step 12799: 3.3624343872070312\n",
      "Loss at step 12819: 0.0\n",
      "Loss at step 12839: 7.799732208251953\n",
      "Loss at step 12859: 0.0\n",
      "Loss at step 12879: 0.0\n",
      "Loss at step 12899: 9.731619834899902\n",
      "Average loss at epoch 29: 3.834926734323245\n",
      "Took: 13.498510360717773 seconds\n",
      "Accuracy at epoch 29: 0.9825 \n",
      "Took: 0.7468070983886719 seconds\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import utils\n",
    "import os\n",
    "import time\n",
    "class ConvNet(object):\n",
    "    def __init__(self):\n",
    "        self.lr = 0.001\n",
    "        self.batch_size = 128\n",
    "        self.keep_prob = tf.constant(0.75)\n",
    "        self.gstep = tf.Variable(0, dtype=tf.int32, \n",
    "                                trainable=False, name='global_step')\n",
    "        self.n_classes = 10\n",
    "        self.skip_step = 20\n",
    "        self.n_test = 10000\n",
    "        self.training = True\n",
    "\n",
    "    def get_data(self):\n",
    "        with tf.name_scope('data'):\n",
    "            train_data, test_data = utils.get_mnist_dataset(self.batch_size)\n",
    "            iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                                   train_data.output_shapes)\n",
    "            img, self.label = iterator.get_next()\n",
    "            self.img = tf.reshape(img, shape=[-1, 28, 28, 1])\n",
    "            # reshape the image to make it work with tf.nn.conv2d\n",
    "\n",
    "            self.train_init = iterator.make_initializer(train_data)  # initializer for train_data\n",
    "            self.test_init = iterator.make_initializer(test_data)    # initializer for train_data\n",
    "\n",
    "    def inference(self):\n",
    "        conv1 = conv_relu(inputs=self.img,\n",
    "                        filters=32,\n",
    "                        k_size=5,\n",
    "                        stride=1,\n",
    "                        padding='SAME',\n",
    "                        scope_name='conv1')\n",
    "        print(\"conv:\"+str(conv1))\n",
    "        pool1 = maxpool(conv1, 2, 2, 'VALID', 'pool1')\n",
    "        print(\"pool1:\"+str(pool1))\n",
    "        conv2 = conv_relu(inputs=pool1,\n",
    "                        filters=64,\n",
    "                        k_size=5,\n",
    "                        stride=1,\n",
    "                        padding='SAME',\n",
    "                        scope_name='conv2')\n",
    "        print(\"conv2:\"+str(conv2))\n",
    "        pool2 = maxpool(conv2, 2, 2, 'VALID', 'pool2')\n",
    "        print(\"pool2:\"+str(pool2))\n",
    "        feature_dim = pool2.shape[1] * pool2.shape[2] * pool2.shape[3]\n",
    "        print(\"feature_dim:\"+str(feature_dim))\n",
    "        pool2 = tf.reshape(pool2, [-1, feature_dim])\n",
    "        print(\"pool2:\"+str(pool2))\n",
    "        fc = fully_connected(pool2, 1024, 'fc')\n",
    "        print(\"fc:\"+str(fc))\n",
    "        dropout = tf.nn.dropout(tf.nn.relu(fc), self.keep_prob, name='relu_dropout')\n",
    "        print(\"dropout:\"+str(dropout))\n",
    "        self.logits = fully_connected(dropout, self.n_classes, 'logits')\n",
    "        print(\"self.logits:\"+str(self.logits))\n",
    "\n",
    "    def loss(self):\n",
    "        '''\n",
    "        define loss function\n",
    "        use softmax cross entropy with logits as the loss function\n",
    "        compute mean cross entropy, softmax is applied internally\n",
    "        '''\n",
    "        # \n",
    "        with tf.name_scope('loss'):\n",
    "            entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.label, logits=self.logits)\n",
    "            self.loss = tf.reduce_mean(entropy, name='loss')\n",
    "    \n",
    "    def optimize(self):\n",
    "        '''\n",
    "        Define training op\n",
    "        using Adam Gradient Descent to minimize cost\n",
    "        '''\n",
    "        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss, \n",
    "                                                global_step=self.gstep)\n",
    "\n",
    "    def summary(self):\n",
    "        '''\n",
    "        Create summaries to write on TensorBoard\n",
    "        '''\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            tf.summary.scalar('accuracy', self.accuracy)\n",
    "            tf.summary.histogram('histogram loss', self.loss)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    def eval(self):\n",
    "        '''\n",
    "        Count the number of right predictions in a batch\n",
    "        '''\n",
    "        with tf.name_scope('predict'):\n",
    "            preds = tf.nn.softmax(self.logits)\n",
    "            correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(self.label, 1))\n",
    "            self.accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "    def build(self):\n",
    "        '''\n",
    "        Build the computation graph\n",
    "        '''\n",
    "        self.get_data()\n",
    "        self.inference()\n",
    "        self.loss()\n",
    "        self.optimize()\n",
    "        self.eval()\n",
    "        self.summary()\n",
    "\n",
    "    def train_one_epoch(self, sess, saver, init, writer, epoch, step):\n",
    "        start_time = time.time()\n",
    "        sess.run(init) \n",
    "        self.training = True\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        try:\n",
    "            while True:\n",
    "                _, l, summaries = sess.run([self.opt, self.loss, self.summary_op])\n",
    "                writer.add_summary(summaries, global_step=step)\n",
    "                if (step + 1) % self.skip_step == 0:\n",
    "                    print('Loss at step {0}: {1}'.format(step, l))\n",
    "                step += 1\n",
    "                total_loss += l\n",
    "                n_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        saver.save(sess, 'checkpoints/convnet_mnist/mnist-convnet', step)\n",
    "        print('Average loss at epoch {0}: {1}'.format(epoch, total_loss/n_batches))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "        return step\n",
    "\n",
    "    def eval_once(self, sess, init, writer, epoch, step):\n",
    "        start_time = time.time()\n",
    "        sess.run(init)\n",
    "        self.training = False\n",
    "        total_correct_preds = 0\n",
    "        try:\n",
    "            while True:\n",
    "                accuracy_batch, summaries = sess.run([self.accuracy, self.summary_op])\n",
    "                writer.add_summary(summaries, global_step=step)\n",
    "                total_correct_preds += accuracy_batch\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        print('Accuracy at epoch {0}: {1} '.format(epoch, total_correct_preds/self.n_test))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        '''\n",
    "        The train function alternates between training one epoch and evaluating\n",
    "        '''\n",
    "        utils.safe_mkdir('checkpoints')\n",
    "        utils.safe_mkdir('checkpoints/convnet_mnist')\n",
    "        writer = tf.summary.FileWriter('./graphs/convnet', tf.get_default_graph())\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/convnet_mnist/checkpoint'))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "            step = self.gstep.eval()\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                step = self.train_one_epoch(sess, saver, self.train_init, writer, epoch, step)\n",
    "                self.eval_once(sess, self.test_init, writer, epoch, step)\n",
    "        writer.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = ConvNet()\n",
    "    model.build()\n",
    "    model.train(n_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
